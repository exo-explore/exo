"""MTP Speculative Decoding for DeepSeek V3.

This module implements speculative decoding using the Multi-Token Prediction (MTP)
layer from DeepSeek V3. The key difference from standard speculative decoding is
that MTP requires hidden states from the main model, not just token predictions.

Based on vLLM/SGLang research:
- 81-82% acceptance rate with k=1
- 1.5-2x speedup at low QPS
"""

import functools
import time
from collections.abc import Callable, Generator
from dataclasses import dataclass
from typing import Any, cast

import mlx.core as mx
import mlx.nn as nn
from mlx_lm.models import cache
from mlx_lm.models.cache import KVCache
from mlx_lm.tokenizer_utils import TokenizerWrapper

from exo.worker.engines.mlx.mtp.module import MTPModule

# Generation stream for async operations
generation_stream = mx.new_stream(mx.default_device())


@dataclass
class MTPGenerationResponse:
    """Response from MTP speculative generation.

    Attributes:
        text: The next segment of decoded text.
        token: The next token.
        logprobs: A vector of log probabilities.
        from_draft: Whether the token was generated by the MTP draft module.
        prompt_tokens: The number of tokens in the prompt.
        prompt_tps: The prompt processing tokens-per-second.
        generation_tokens: The number of generated tokens.
        generation_tps: The tokens-per-second for generation.
        peak_memory: The peak memory used so far in GB.
        finish_reason: The reason the response is being sent: "length", "stop" or None.
    """

    text: str
    token: int
    logprobs: mx.array
    from_draft: bool
    prompt_tokens: int
    prompt_tps: float
    generation_tokens: int
    generation_tps: float
    peak_memory: float
    finish_reason: str | None = None


def maybe_quantize_kv_cache(
    prompt_cache: list[Any],
    quantized_kv_start: int,
    kv_group_size: int,
    kv_bits: int | None,
) -> None:
    """Quantize KV cache entries if needed."""
    if kv_bits is None:
        return
    for e, c in enumerate(prompt_cache):
        if (
            hasattr(c, "to_quantized")
            and hasattr(c, "offset")
            and c.offset >= quantized_kv_start
        ):
            prompt_cache[e] = c.to_quantized(group_size=kv_group_size, bits=kv_bits)


class ModelWithHiddenStates(nn.Module):
    """Wrapper to extract hidden states before lm_head.

    This wrapper allows capturing the hidden states from the transformer
    layers before the final lm_head projection, which is needed for MTP.
    """

    def __init__(self, base_model: nn.Module) -> None:
        super().__init__()
        self._base = base_model

    def forward_with_hidden(
        self,
        inputs: mx.array,
        model_cache: list[Any] | None = None,
    ) -> tuple[mx.array, mx.array]:
        """Forward pass that returns both logits and hidden states.

        Args:
            inputs: Input token ids
            model_cache: KV cache

        Returns:
            Tuple of (logits, hidden_states)
        """
        # Call the inner model (transformer layers + norm)
        hidden: mx.array = self._base.model(inputs, model_cache)
        # Get logits from lm_head
        logits: mx.array = self._base.lm_head(hidden)
        return logits, hidden

    def forward(
        self,
        inputs: mx.array,
        model_cache: list[Any] | None = None,
    ) -> mx.array:
        """Standard forward pass returning only logits."""
        return cast(mx.array, self._base(inputs, cache=model_cache))

    @property
    def layers(self) -> list[nn.Module]:
        """Access layers for cache creation."""
        return cast(list[nn.Module], self._base.layers)


def mtp_speculative_generate_step(
    prompt: mx.array,
    model: nn.Module,
    mtp_module: MTPModule,
    *,
    num_draft_tokens: int = 1,
    max_tokens: int = 256,
    sampler: Callable[[mx.array], mx.array] | None = None,
    logits_processors: list[Callable[[mx.array, mx.array], mx.array]] | None = None,
    prompt_cache: list[Any] | None = None,
    mtp_cache: KVCache | None = None,
    prefill_step_size: int = 512,
    kv_bits: int | None = None,
    kv_group_size: int = 64,
    quantized_kv_start: int = 0,
) -> Generator[tuple[int, mx.array, bool], None, None]:
    """MTP speculative decoding generator.

    Unlike standard speculative decoding where the draft model only needs tokens,
    MTP requires the hidden states from the main model. This generator:

    1. Runs the main model to get logits AND hidden states
    2. Uses MTP module with hidden state + sampled token to predict next token
    3. Verifies MTP predictions with the main model
    4. Accepts/rejects based on matching

    Args:
        prompt: The input prompt as token ids
        model: The main model (must support return_hidden=True)
        mtp_module: The MTP module for draft prediction
        num_draft_tokens: Number of tokens to draft (typically 1 for MTP)
        max_tokens: Maximum number of tokens to generate
        sampler: Optional sampler function for token selection
        logits_processors: Optional list of logits processors
        prompt_cache: KV cache for the main model
        mtp_cache: KV cache for the MTP module
        prefill_step_size: Step size for prompt processing
        kv_bits: Bits for KV cache quantization
        kv_group_size: Group size for KV cache quantization
        quantized_kv_start: Step to begin cache quantization

    Yields:
        Tuple of (token, logprobs, from_draft)
    """
    y = prompt.astype(mx.uint32)
    prev_tokens: mx.array | None = None

    # Wrap model to get hidden states
    wrapped_model = (
        model
        if isinstance(model, ModelWithHiddenStates)
        else ModelWithHiddenStates(model)
    )

    # Create caches if needed
    if prompt_cache is None:
        prompt_cache = cache.make_prompt_cache(model)
    if mtp_cache is None:
        mtp_cache = KVCache()

    final_sampler = (
        sampler if sampler is not None else (lambda x: mx.argmax(x, axis=-1))
    )

    quantize_cache_fn = functools.partial(
        maybe_quantize_kv_cache,
        quantized_kv_start=quantized_kv_start,
        kv_group_size=kv_group_size,
        kv_bits=kv_bits,
    )

    def _process_and_sample(
        tokens: mx.array | None,
        logits: mx.array,
    ) -> tuple[mx.array, mx.array]:
        """Process logits and sample tokens."""
        nonlocal logits_processors
        processed_logits = logits
        if logits_processors:
            for processor in logits_processors:
                processed_logits = processor(
                    tokens if tokens is not None else mx.array([]), processed_logits
                )

        logprobs = processed_logits - mx.logsumexp(
            processed_logits, axis=-1, keepdims=True
        )
        sampled = final_sampler(logprobs)
        return sampled, logprobs

    def _main_model_step_with_hidden(
        input_y: mx.array,
    ) -> tuple[mx.array, mx.array, mx.array]:
        """Run main model step with hidden state return."""
        nonlocal prev_tokens

        with mx.stream(generation_stream):
            logits, hidden = wrapped_model.forward_with_hidden(
                input_y[None], prompt_cache
            )
            logits = logits[:, -1, :]
            quantize_cache_fn(prompt_cache)

            if logits_processors:
                prev_tokens = (
                    mx.concatenate([prev_tokens, input_y])
                    if prev_tokens is not None
                    else input_y
                )

            sampled, logprobs_result = _process_and_sample(prev_tokens, logits)
            return sampled, logprobs_result.squeeze(0), hidden[:, -1:, :]

    def _main_model_step(
        input_y: mx.array,
    ) -> tuple[mx.array, mx.array]:
        """Run main model step without hidden state."""
        nonlocal prev_tokens

        with mx.stream(generation_stream):
            logits = wrapped_model.forward(input_y[None], prompt_cache)
            logits = logits[:, -1, :]
            quantize_cache_fn(prompt_cache)

            if logits_processors:
                prev_tokens = (
                    mx.concatenate([prev_tokens, input_y])
                    if prev_tokens is not None
                    else input_y
                )

            sampled, logprobs_result = _process_and_sample(prev_tokens, logits)
            return sampled, logprobs_result.squeeze(0)

    def _mtp_draft(
        hidden_state: mx.array,
        draft_token: mx.array,
    ) -> tuple[mx.array, mx.array]:
        """Generate draft token using MTP module."""
        with mx.stream(generation_stream):
            logits, new_hidden = mtp_module(
                hidden_state,
                draft_token,
                cache=mtp_cache,
            )
            logits = logits[:, -1, :]
            sampled, _ = _process_and_sample(None, logits)
            return sampled, new_hidden

    def _prefill(input_y: mx.array) -> mx.array:
        """Prefill the prompt cache."""
        result_y = input_y
        while result_y.size > prefill_step_size:
            _ = wrapped_model.forward(result_y[:prefill_step_size][None], prompt_cache)
            quantize_cache_fn(prompt_cache)
            mx.eval([c.state for c in prompt_cache])
            result_y = result_y[prefill_step_size:]
            mx.clear_cache()
        return result_y

    def _rewind_cache(num_draft: int, num_accept: int) -> None:
        """Rewind caches after rejection."""
        cache.trim_prompt_cache(prompt_cache, num_draft - num_accept)

    # Prefill phase
    with mx.stream(generation_stream):
        y = _prefill(y)

    ntoks = 0
    num_draft = 0
    n_accepted = 0
    last_hidden: mx.array | None = None

    try:
        # Initial step to get first token and hidden state
        sampled, logprobs, last_hidden = _main_model_step_with_hidden(y)
        mx.eval(sampled, logprobs, last_hidden)

        y = sampled
        current_logprobs = logprobs

        while ntoks < max_tokens:
            # Draft phase: use MTP to predict next token
            num_draft = min(max_tokens - ntoks - 1, num_draft_tokens)

            if num_draft > 0 and last_hidden is not None:
                # Use MTP to draft
                draft_token, draft_hidden = _mtp_draft(last_hidden, y)
                mx.eval(draft_token, draft_hidden)

                # Verify with main model
                # Feed the drafted token to main model
                verify_input = mx.concatenate([y, draft_token.flatten()])
                verify_sampled, verify_logprobs, new_hidden = (
                    _main_model_step_with_hidden(verify_input)
                )
                mx.eval(verify_sampled, verify_logprobs, new_hidden)

                # Check if draft matches verification
                draft_token_val = int(draft_token.item())
                verify_token_val = (
                    int(verify_sampled[0].item())
                    if verify_sampled.shape[0] > 1
                    else int(verify_sampled.item())
                )

                # Yield the current token (not from draft)
                ntoks += 1
                yield int(y.item()), current_logprobs, False

                if ntoks >= max_tokens:
                    break

                if draft_token_val == verify_token_val:
                    # Draft accepted
                    n_accepted += 1
                    ntoks += 1
                    draft_logprobs = (
                        verify_logprobs[0]
                        if verify_logprobs.ndim > 1
                        else verify_logprobs
                    )
                    yield draft_token_val, draft_logprobs, True

                    if ntoks >= max_tokens:
                        break

                    # Continue with the token after the draft
                    y = (
                        verify_sampled[-1:]
                        if verify_sampled.ndim > 0 and verify_sampled.shape[0] > 1
                        else verify_sampled
                    )
                    current_logprobs = (
                        verify_logprobs[-1]
                        if verify_logprobs.ndim > 1
                        else verify_logprobs
                    )
                    last_hidden = new_hidden
                else:
                    # Draft rejected - rewind and use verified token
                    _rewind_cache(1, 0)
                    y = (
                        verify_sampled[:1]
                        if verify_sampled.ndim > 0 and verify_sampled.shape[0] > 1
                        else verify_sampled
                    )
                    current_logprobs = (
                        verify_logprobs[0]
                        if verify_logprobs.ndim > 1
                        else verify_logprobs
                    )
                    last_hidden = (
                        new_hidden[:, :1, :] if new_hidden is not None else None
                    )
            else:
                # No drafting, just do normal generation
                ntoks += 1
                yield int(y.item()), current_logprobs, False

                if ntoks >= max_tokens:
                    break

                sampled, logprobs, last_hidden = _main_model_step_with_hidden(y)
                mx.eval(sampled, logprobs, last_hidden)

                y = sampled
                current_logprobs = logprobs

            if ntoks % 256 == 0:
                mx.clear_cache()

    finally:
        _rewind_cache(num_draft, n_accepted)


def mtp_speculative_generate(
    model: nn.Module,
    mtp_module: MTPModule,
    tokenizer: TokenizerWrapper,
    prompt: str | mx.array | list[int],
    max_tokens: int = 256,
    sampler: Callable[[mx.array], mx.array] | None = None,
    logits_processors: list[Callable[[mx.array, mx.array], mx.array]] | None = None,
    prompt_cache: list[Any] | None = None,
    num_draft_tokens: int = 1,
    prefill_step_size: int = 512,
    kv_group_size: int = 64,
    kv_bits: int | None = None,
) -> Generator[MTPGenerationResponse, None, None]:
    """High-level MTP speculative generation with text output.

    Args:
        model: The main model
        mtp_module: The MTP module for draft prediction
        tokenizer: Tokenizer for encoding/decoding
        prompt: Input prompt (string, array, or token list)
        max_tokens: Maximum tokens to generate
        sampler: Optional sampler function
        logits_processors: Optional logits processors
        prompt_cache: Optional KV cache
        num_draft_tokens: Number of draft tokens
        prefill_step_size: Prefill step size
        kv_group_size: KV group size
        kv_bits: KV bits

    Yields:
        MTPGenerationResponse objects with text and metadata
    """
    if not isinstance(prompt, mx.array):
        if isinstance(prompt, str):
            bos_token = getattr(tokenizer, "bos_token", None)
            add_special_tokens = bos_token is None or not prompt.startswith(
                str(bos_token)
            )
            encoded: list[int] = tokenizer.encode(
                prompt, add_special_tokens=add_special_tokens
            )
            prompt = mx.array(encoded)
        else:
            prompt = mx.array(prompt)

    detokenizer = tokenizer.detokenizer
    eos_token_ids: list[int] = getattr(tokenizer, "eos_token_ids", [])

    token_generator = mtp_speculative_generate_step(
        prompt,
        model,
        mtp_module,
        max_tokens=max_tokens,
        sampler=sampler,
        logits_processors=logits_processors,
        prompt_cache=prompt_cache,
        num_draft_tokens=num_draft_tokens,
        prefill_step_size=prefill_step_size,
        kv_group_size=kv_group_size,
        kv_bits=kv_bits,
    )

    tic = time.perf_counter()
    prompt_tps = 0.0
    token = 0
    logprobs: mx.array = mx.array([0.0])
    from_draft = False
    n = 0

    for n, (token, logprobs, from_draft) in enumerate(token_generator):
        if n == 0:
            prompt_time = time.perf_counter() - tic
            prompt_tps = float(prompt.size) / prompt_time
            tic = time.perf_counter()

        if token in eos_token_ids:
            break

        detokenizer.add_token(token)
        if (n + 1) == max_tokens:
            break

        yield MTPGenerationResponse(
            text=str(detokenizer.last_segment),
            token=token,
            logprobs=logprobs,
            from_draft=from_draft,
            prompt_tokens=int(prompt.size),
            prompt_tps=prompt_tps,
            generation_tokens=n + 1,
            generation_tps=(n + 1) / (time.perf_counter() - tic),
            peak_memory=mx.get_peak_memory() / 1e9,
            finish_reason=None,
        )

    detokenizer.finalize()
    yield MTPGenerationResponse(
        text=str(detokenizer.last_segment),
        token=token,
        logprobs=logprobs,
        from_draft=from_draft,
        prompt_tokens=int(prompt.size),
        prompt_tps=prompt_tps,
        generation_tokens=n + 1,
        generation_tps=(n + 1) / (time.perf_counter() - tic),
        peak_memory=mx.get_peak_memory() / 1e9,
        finish_reason="stop" if token in eos_token_ids else "length",
    )
