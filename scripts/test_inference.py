#!/usr/bin/env python3
"""
Simple test script to verify llama.cpp inference works on Termux/Android.

Usage:
    python3 scripts/test_inference.py
    python3 scripts/test_inference.py --model /path/to/model.gguf
"""

import os
import sys
import glob
import argparse
from pathlib import Path


def find_model() -> Path | None:
    """Find a GGUF model in the default exo models directory."""
    models_dir = Path.home() / ".exo" / "models"
    
    if not models_dir.exists():
        return None
    
    # Search for .gguf files recursively
    for gguf in models_dir.rglob("*.gguf"):
        return gguf
    
    return None


def get_system_info() -> dict[str, str]:
    """Get basic system information."""
    info = {}
    
    try:
        import platform
        info["system"] = platform.system()
        info["machine"] = platform.machine()
        info["python"] = platform.python_version()
    except Exception:
        pass
    
    try:
        import psutil
        mem = psutil.virtual_memory()
        info["ram_gb"] = f"{mem.total / (1024**3):.1f}"
        info["ram_available_gb"] = f"{mem.available / (1024**3):.1f}"
    except Exception:
        pass
    
    return info


def test_inference(model_path: Path) -> bool:
    """Run a simple inference test."""
    print(f"\nüîÑ Loading model: {model_path.name}")
    print(f"   Full path: {model_path}")
    print(f"   Size: {model_path.stat().st_size / (1024**2):.1f} MB")
    
    try:
        from llama_cpp import Llama
        
        # Load model with conservative settings for Android
        print("\n‚è≥ Initializing model (this may take a moment)...")
        
        llm = Llama(
            model_path=str(model_path),
            n_ctx=512,           # Small context for testing
            n_threads=4,         # Conservative thread count
            n_gpu_layers=0,      # CPU only
            verbose=False,
        )
        
        print("‚úì Model loaded successfully!")
        
        # Run a simple inference
        print("\nü§ñ Running test inference...")
        prompt = "Hello! Please respond with a single short sentence."
        
        output = llm(
            prompt,
            max_tokens=32,
            temperature=0.7,
            stop=["\n", "."],
        )
        
        response = output["choices"][0]["text"].strip()
        
        print(f"\nüìù Prompt: {prompt}")
        print(f"üí¨ Response: {response}")
        
        # Show usage stats
        if "usage" in output:
            usage = output["usage"]
            print(f"\nüìä Tokens - Prompt: {usage.get('prompt_tokens', '?')}, "
                  f"Generated: {usage.get('completion_tokens', '?')}")
        
        print("\n‚úÖ Inference test PASSED!")
        return True
        
    except ImportError as e:
        print(f"\n‚ùå llama-cpp-python not installed: {e}")
        print("   Try: pip install llama-cpp-python --no-cache-dir")
        return False
        
    except Exception as e:
        print(f"\n‚ùå Inference failed: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(description="Test llama.cpp inference on Termux")
    parser.add_argument("--model", "-m", type=str, help="Path to GGUF model file")
    args = parser.parse_args()
    
    print("=" * 50)
    print("   exo llama.cpp Inference Test")
    print("=" * 50)
    
    # Show system info
    print("\nüì± System Information:")
    info = get_system_info()
    for key, value in info.items():
        print(f"   {key}: {value}")
    
    # Check for llama_cpp
    print("\nüîç Checking llama-cpp-python...")
    try:
        import llama_cpp
        print(f"   ‚úì Version: {llama_cpp.__version__}")
    except ImportError:
        print("   ‚ùå llama-cpp-python not installed")
        print("   Run: pip install llama-cpp-python --no-cache-dir")
        sys.exit(1)
    
    # Find or use specified model
    model_path = None
    
    if args.model:
        model_path = Path(args.model)
        if not model_path.exists():
            print(f"\n‚ùå Model not found: {model_path}")
            sys.exit(1)
    else:
        print("\nüîç Looking for models in ~/.exo/models/...")
        model_path = find_model()
        
        if model_path is None:
            print("   ‚ùå No models found!")
            print("\n   Download a model first:")
            print("   ./scripts/download_model.sh qwen-0.5b")
            print("   ./scripts/download_model.sh tinyllama")
            sys.exit(1)
        
        print(f"   ‚úì Found: {model_path.name}")
    
    # Run inference test
    success = test_inference(model_path)
    
    if success:
        print("\n" + "=" * 50)
        print("   üéâ All tests passed! exo is ready for inference.")
        print("=" * 50)
    else:
        print("\n" + "=" * 50)
        print("   ‚ö†Ô∏è  Some tests failed. Check the errors above.")
        print("=" * 50)
        sys.exit(1)


if __name__ == "__main__":
    main()

