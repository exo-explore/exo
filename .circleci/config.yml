version: 2.1

orbs:
  python: circleci/python@2
  win: circleci/windows@5.0
commands:
  run_chatgpt_api_test:
    parameters:
      inference_engine:
        type: string
      model_id:
        type: string
      expected_output:
        type: string
      prompt:
        type: string
    steps:
      - run:
          name: Run chatgpt api integration test (<<parameters.inference_engine>>, <<parameters.model_id>>)
          command: |
            source env/bin/activate

            # Start first instance
            HF_HOME="$(pwd)/.hf_cache_node1" DEBUG_DISCOVERY=7 DEBUG=7 exo --inference-engine <<parameters.inference_engine>> --node-id "node1" --listen-port 5678 --broadcast-port 5679 --chatgpt-api-port 8000 --chatgpt-api-response-timeout 900 2>&1 | tee output1.log &
            PID1=$!

            # Start second instance
            HF_HOME="$(pwd)/.hf_cache_node2" DEBUG_DISCOVERY=7 DEBUG=7 exo --inference-engine <<parameters.inference_engine>> --node-id "node2" --listen-port 5679 --broadcast-port 5678 --chatgpt-api-port 8001 --chatgpt-api-response-timeout 900 2>&1 | tee output2.log &
            PID2=$!

            # Wait for discovery
            sleep 10

            # Function to check if processes are still running
            check_processes() {
              if ! kill -0 $PID1 2>/dev/null; then
                echo "First instance (PID $PID1) died unexpectedly. Log output:"
                cat output1.log
                exit 1
              fi
              if ! kill -0 $PID2 2>/dev/null; then
                echo "Second instance (PID $PID2) died unexpectedly. Log output:"
                cat output2.log
                exit 1
              fi
            }

            # Check processes before proceeding
            check_processes

            # Special handling for dummy engine
            if [ "<<parameters.inference_engine>>" = "dummy" ]; then
              expected_content="This is a dummy response"
            else
              expected_content="Michael Jackson"
            fi

            echo "Sending request to first instance..."
            response_1=$(curl -s http://localhost:8000/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "<<parameters.model_id>>",
                "messages": [{"role": "user", "content": "<<parameters.prompt>>"}],
                "temperature": 0.7
              }')
            echo "Response 1: $response_1"

            # Check processes after first response
            check_processes

            echo "Sending request to second instance..."
            response_2=$(curl -s http://localhost:8001/v1/chat/completions \
              -H "Content-Type: application/json" \
              -d '{
                "model": "<<parameters.model_id>>",
                "messages": [{"role": "user", "content": "<<parameters.prompt>>"}],
                "temperature": 0.7
              }')
            echo "Response 2: $response_2"

            # Check processes after second response
            check_processes

            # Stop both instances
            kill $PID1 $PID2

            echo ""
            if ! echo "$response_1" | grep -q "<<parameters.expected_output>>" || ! echo "$response_2" | grep -q "<<parameters.expected_output>>"; then
              echo "Test failed: Response does not contain '<<parameters.expected_output>>'"
              echo "Response 1: $response_1"
              echo ""
              echo "Response 2: $response_2"
              echo "Output of first instance:"
              cat output1.log
              echo "Output of second instance:"
              cat output2.log
              exit 1
            else
              echo "Test passed: Response from both nodes contains '<<parameters.expected_output>>'"
            fi

jobs:
  unit_test:
    macos:
      xcode: "16.0.0"
    resource_class: m2pro.large
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run:
          name: Run tests
          command: |
            source env/bin/activate
            # set TEMPERATURE to 0 for deterministic sampling
            echo "Running inference engine tests..."
            METAL_DEVICE_WRAPPER_TYPE=1 METAL_DEBUG_ERROR_MODE=0 METAL_XCODE=1 TEMPERATURE=0 python3 -m exo.inference.test_inference_engine
            echo "Running tokenizer tests..."
            python3 ./test/test_tokenizers.py

  discovery_integration_test:
    macos:
      xcode: "16.0.0"
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run:
          name: Run discovery integration test
          command: |
            source env/bin/activate
            DEBUG_DISCOVERY=7 DEBUG=7 exo --node-id "node1" --listen-port 5678 --broadcast-port 5679 --chatgpt-api-port 8000 > output1.log 2>&1 &
            PID1=$!
            DEBUG_DISCOVERY=7 DEBUG=7 exo --node-id "node2" --listen-port 5679 --broadcast-port 5678 --chatgpt-api-port 8001 > output2.log 2>&1 &
            PID2=$!
            sleep 10
            kill $PID1 $PID2
            if grep -q "Peer statuses: {\\'node2\\': \\'is_connected=True, health_check=True" output1.log && ! grep -q "Failed to connect peers:" output1.log && grep -q "Peer statuses: {\\'node1\\': \\'is_connected=True, health_check=True" output2.log && ! grep -q "Failed to connect peers:" output2.log; then
              echo "Test passed: Both instances discovered each other"
              exit 0
            else
              echo "Test failed: Devices did not discover each other"
              echo "Output of first instance:"
              cat output1.log
              echo "Output of second instance:"
              cat output2.log
              exit 1
            fi

  chatgpt_api_integration_test_mlx:
    macos:
      xcode: "16.0.0"
    resource_class: m2pro.large
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run_chatgpt_api_test:
          inference_engine: mlx
          model_id: llama-3.2-1b
          prompt: "Keep responses concise. Who was the king of pop?"
          expected_output: "Michael Jackson"

  chatgpt_api_integration_test_dummy:
    macos:
      xcode: "16.0.0"
    resource_class: m2pro.large
    steps:
      - checkout
      - run:
          name: Set up Python
          command: |
            brew install python@3.12
            python3.12 -m venv env
            source env/bin/activate
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run_chatgpt_api_test:
          inference_engine: dummy
          model_id: dummy-model
          prompt: "Dummy prompt."
          expected_output: "dummy"

  test_macos_m1:
    macos:
      xcode: "16.0.0"
    resource_class: m2pro.large
    steps:
      - checkout
      - run: system_profiler SPHardwareDataType

  # chatgpt_api_integration_test_tinygrad:
  #   macos:
  #     xcode: "16.0.0"
  #   resource_class: m2pro.large
  #   steps:
  #     - checkout
  #     - run:
  #         name: Set up Python
  #         command: |
  #           brew install python@3.12
  #           python3.12 -m venv env
  #           source env/bin/activate
  #     - run:
  #         name: Install dependencies
  #         command: |
  #           source env/bin/activate
  #           pip install --upgrade pip
  #           pip install .
  #     - run_chatgpt_api_test:
  #         inference_engine: tinygrad
  #         model_id: llama-3-8b
  #         prompt: "Keep responses concise. Who was the king of pop?"
  #         expected_output: "Michael Jackson"

  build_exo_macos:
    macos:
      xcode: "16.0.0"
    resource_class: m2pro.large
    steps:
      - checkout
      - run:
          name: Set up Python environment
          command: |
            brew install python@3.12
            brew install npm
            python3.12 -m venv env
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run:
          name: Build exo for macOS
          command: |
            source env/bin/activate
            python3 buildexo/build_exo.py
      - run:
          name: Move exo executable to exo-app
          command: |
            mkdir -p ./exo/exo-app/backend
            mv ./dist/main.dist/exo ./exo/exo-app/backend/exo
      - run:
          name: package electron app
          command:
            cd ./exo/exo-app
            npm install
            npm run make
            mv ./out/make/zip/darwin/arm64/exo-darwin-arm64-*.zip ../../
      - persist_to_workspace:
          root: .
          paths:
            - ./exo-darwin-arm64-*.zip

  build_exo_linux:
    machine:
      image: ubuntu-2004:current
    resource_class: xlarge
    steps:
      - checkout
      - run:
          name: Set up Python environment
          command: |
            sudo apt-get update
            sudo apt-get install -y python3 python3-venv
            sudo apt-get install clang
            sudo apt-get install patchelf
            python3 -m venv env
      - run:
          name: Install dependencies
          command: |
            source env/bin/activate
            pip install --upgrade pip
            pip install .
      - run:
          name: Build exo for Linux
          no_output_timeout: 90m
          command: |
            source env/bin/activate
            python3 buildexo/build_exo.py
      - run:
          name: Create an AppImage
          command: |
            sudo wget https://github.com/AppImage/AppImageKit/releases/download/continuous/appimagetool-x86_64.AppImage -O /usr/local/bin/appimagetool
            sudo chmod +x /usr/local/bin/appimagetool
            mkdir -p exo.AppDir/usr/bin
            cp -r env/lib/python3.12/site-packages/tinygrad exo.AppDir/usr/bin
            cp -r dist/main.dist/* exo.AppDir/usr/bin
            # copying icons from docs to exo.AppDir
            cp docs/exo-rounded.png exo.AppDir
            cat > exo.AppDir/exo.desktop << 'EOF'
            [Desktop Entry]
            Name=exo
            Icon=exo-rounded
            Terminal=false
            Type=Application
            Categories=Utility;
            EOF

            cat > exo.AppDir/AppRun << 'EOF'
            #!/bin/bash
            export APPDIR="$(dirname "$(readlink -f "$0")")"
            export PATH="$APPDIR/usr/bin/:$PATH"
            export LD_LIBRARY_PATH="$APPDIR/usr/lib:$LD_LIBRARY_PATH"
            export XDG_DATA_DIRS="$APPDIR/usr/share/:/usr/share/:$XDG_DATA_DIRS"
            "$APPDIR"/usr/bin/exo "$@"
            EOF
            
            chmod +x exo.AppDir/AppRun
            appimagetool exo.AppDir/
            mv *.AppImage ./output/
                      
  build_exo_windows:
    executor: win/default
    steps:
      - checkout
      - run:
          name: Install Conda
          command: |
            choco install anaconda3 --yes
      - run:
          name: Set up Conda environment
          command: |
            conda create -n exo-env python=3.12 --yes
            conda activate exo-env
            conda install -c conda-forge python=3.12 
            conda install -c conda-forge libstatic
            conda install -c conda-forge rust
      - run:
          name: Install dependencies
          command: |
            conda activate exo-env
            pip install --upgrade pip
            pip install .
      - run:
          name: Build Exo for Windows
          command: |
            conda activate exo-env
            python buildexo/build_exo.py

  publish_github_release:
    docker:
      - image: circleci/golang:1.8
    steps:
      - attach_workspace:
          at: .
      - run:
          name: Publish Release on GitHub
          command: |
            echo "Publish GitHub release"
            go install github.com/tcnksm/ghr@latest
            ghr -t ${GITHUB_TOKEN} -u ${CIRCLE_PROJECT_USERNAME} -r ${CIRCLE_PROJECT_REPONAME} \
              -c ${CIRCLE_SHA1} -delete ${VERSION} ./exo/exo-app/exo-darwin-arm64.zip

workflows:
  version: 2
  build_and_test:
    jobs:
      - unit_test
      - discovery_integration_test
      - chatgpt_api_integration_test_mlx
      - chatgpt_api_integration_test_dummy
      - test_macos_m1
      - chatgpt_api_integration_test_tinygrad
      - build_exo_macos:
        filters:
          tags:
            only: /.*/
      - publish_github_release:
          requires:
            - build_exo_macos
          filters:
            tags:
              only: /^v.*/
            branches:
              ignore: /.*/
