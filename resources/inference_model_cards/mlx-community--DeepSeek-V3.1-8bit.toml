model_id = "mlx-community/DeepSeek-V3.1-8bit"
revision = "cd6c63546a6d33a8cc75158dc60d1746787306ac"
n_layers = 61
hidden_size = 7168
supports_tensor = true
tasks = ["TextGeneration"]
family = "deepseek"
quantization = "8bit"
base_model = "DeepSeek V3.1"
capabilities = ["text", "thinking", "thinking_toggle"]

[storage_size]
in_bytes = 765577920512
