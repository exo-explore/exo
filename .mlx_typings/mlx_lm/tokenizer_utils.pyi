"""
This type stub file was generated by pyright.
"""

from functools import partial
from pathlib import Path
from transformers import PreTrainedTokenizerFast

"""
This type stub file was generated by pyright.
"""

class StreamingDetokenizer:
    """The streaming detokenizer interface so that we can detokenize one token at a time.

    Example usage is as follows:

        detokenizer = ...

        # Reset the tokenizer state
        detokenizer.reset()

        for token in generate(...):
            detokenizer.add_token(token.item())

            # Contains the whole text so far. Some tokens may not be included
            # since it contains whole words usually.
            detokenizer.text

            # Contains the printable segment (usually a word) since the last
            # time it was accessed
            detokenizer.last_segment

            # Contains all the tokens added so far
            detokenizer.tokens

        # Make sure that we detokenize any remaining tokens
        detokenizer.finalize()

        # Now detokenizer.text should match tokenizer.decode(detokenizer.tokens)
    """

    __slots__ = ...
    def reset(self): ...
    def add_token(self, token): ...
    def finalize(self): ...
    @property
    def last_segment(self):
        """Return the last segment of readable text since last time this property was accessed."""
        ...

class NaiveStreamingDetokenizer(StreamingDetokenizer):
    """NaiveStreamingDetokenizer relies on the underlying tokenizer
    implementation and should work with every tokenizer.

    Its complexity is O(T^2) where T is the longest line since it will
    repeatedly detokenize the same tokens until a new line is generated.
    """
    def __init__(self, tokenizer) -> None: ...
    def reset(self): ...
    def add_token(self, token): ...
    def finalize(self): ...
    @property
    def text(self): ...

class SPMStreamingDetokenizer(StreamingDetokenizer):
    """A streaming detokenizer for SPM models.

    It adds tokens to the text if the next token starts with the special SPM
    underscore which results in linear complexity.
    """
    def __init__(self, tokenizer, trim_space=...) -> None: ...
    def reset(self): ...
    def add_token(self, token): ...
    def finalize(self): ...

class BPEStreamingDetokenizer(StreamingDetokenizer):
    """A streaming detokenizer for OpenAI style BPE models.

    It adds tokens to the text if the next token starts with a space similar to
    the SPM detokenizer.
    """

    _byte_decoder = ...
    _space_matches = ...
    def __init__(self, tokenizer) -> None: ...
    def reset(self): ...
    def add_token(self, token): ...
    def finalize(self): ...
    @classmethod
    def make_byte_decoder(cls):
        """See https://github.com/openai/gpt-2/blob/master/src/encoder.py for the rationale."""
        ...

class TokenizerWrapper:
    """A wrapper that combines an HF tokenizer and a detokenizer.

    Accessing any attribute other than the ``detokenizer`` is forwarded to the
    huggingface tokenizer.
    """

    bos_token: str | None
    eos_token: str | None
    pad_token: str | None
    def __init__(self, tokenizer, detokenizer_class=..., eos_token_ids=...) -> None: ...
    def encode(self, text: str, add_special_tokens: bool = ...) -> list[int]: ...
    def decode(self, token_ids: list[int], skip_special_tokens: bool = ...) -> str: ...
    def add_eos_token(self, token: str): ...
    @property
    def has_thinking(self): ...
    @property
    def think_start(self): ...
    @property
    def think_end(self): ...
    @property
    def has_tool_calling(self): ...
    @property
    def tool_call_start(self): ...
    @property
    def tool_call_end(self): ...
    @property
    def detokenizer(self):
        """
        Get a stateful streaming detokenizer.
        """
        ...

    def __getattr__(self, attr): ...
    def __setattr__(self, attr, value): ...

class NewlineTokenizer(PreTrainedTokenizerFast):
    """A tokenizer that replaces newlines with <n> and <n> with new line."""
    def __init__(self, *args, **kwargs) -> None: ...
    def encode(self, text, **kwargs): ...
    def encode_batch(self, texts, **kwargs): ...
    def decode(self, *args, **kwargs): ...
    def batch_decode(self, *args, **kwargs): ...

def load_tokenizer(
    model_path: Path,
    tokenizer_config_extra=...,
    return_tokenizer=...,
    eos_token_ids=...,
) -> (
    TokenizerWrapper
    | type[SPMStreamingDetokenizer]
    | partial[SPMStreamingDetokenizer]
    | type[BPEStreamingDetokenizer]
    | type[NaiveStreamingDetokenizer]
):
    """Load a huggingface tokenizer and try to infer the type of streaming
    detokenizer to use.

    Note, to use a fast streaming tokenizer, pass a local file path rather than
    a Hugging Face repo ID.
    """
    ...

def no_bos_or_eos(sequence: list, bos: int, eos: int) -> list: ...
